{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T18:08:49.338570Z",
     "start_time": "2019-04-13T18:08:49.333984Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T08:46:44.435815Z",
     "start_time": "2019-04-13T08:46:44.429644Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_fightmetric(pages=26):\n",
    "    \"\"\"Scrape Fightmetric for all UFC fighter wins and losses.\"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    urls = [f'http://www.fightmetric.com/statistics/fighters?char={char}&page=all' \n",
    "            for char in letters]\n",
    "    content = []\n",
    "    for i in range(pages):\n",
    "        r = requests.get(urls[i])\n",
    "        if r.status_code == 200:\n",
    "            print(letters[i], end='')\n",
    "            content.append(r.text)\n",
    "        else:\n",
    "            print(f'Failed to scrape {urls[i]}\\nstatus: {temp.status_code}')\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T16:18:03.588725Z",
     "start_time": "2019-04-13T16:18:03.581088Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_urls(pages):\n",
    "    \"\"\"Extract urls to fighter pages from list of html soup strings.\"\"\"\n",
    "    ss = SoupStrainer('a', {'class': 'b-link b-link_style_black'})\n",
    "    fighter_urls = set()\n",
    "    for page in pages:\n",
    "        soup = BeautifulSoup(page, 'lxml', parse_only=ss)\n",
    "        for s in soup:\n",
    "            try:\n",
    "                fighter_urls.add(s['href'])\n",
    "            except:\n",
    "                pass\n",
    "    return list(fighter_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T16:19:42.247595Z",
     "start_time": "2019-04-13T16:19:42.240968Z"
    }
   },
   "outputs": [],
   "source": [
    "def attr_parser(soup, dtype, attr, element='li'):\n",
    "    \"\"\"Use in scrape_fighter_page func to retrieve field from page.\"\"\"\n",
    "    current_tag = soup.find(lambda tag: tag.name == element \n",
    "                            and attr in tag.text.lower())\n",
    "    if current_tag and '--' not in current_tag.text:\n",
    "        cleaned_text = list(current_tag)[-1].strip().strip('.').strip('\\\"')\n",
    "        if '%' in cleaned_text:\n",
    "            cleaned_text = dtype(cleaned_text.strip('%')) / 100\n",
    "        if dtype == 'date':\n",
    "            return datetime.datetime.strptime(cleaned_text, '%b %d, %Y')\n",
    "        else:\n",
    "            try:\n",
    "                return dtype(cleaned_text)\n",
    "            except ValueError:\n",
    "                return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T17:04:39.803552Z",
     "start_time": "2019-04-13T17:04:39.772606Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_fighter_page(url, verbose=False):\n",
    "    \"\"\"Pass in list of fighter urls from extract_urls().\n",
    "    Scrape each page for W/L record by year, weight class, nationality,\n",
    "    striking stats, etc.\n",
    "    \"\"\"\n",
    "    sess = requests.Session()\n",
    "    retry = Retry(total=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    sess.mount('http://', adapter)\n",
    "    sess.mount('https://', adapter)\n",
    "    \n",
    "    stats = {}\n",
    "    fields = [(int, 'height'), (int, 'weight'), (int, 'reach'), \n",
    "              (str, 'stance'), ('date', 'dob'), (float, 'slpm'), \n",
    "              (float, 'str. acc'), (float, 'sapm'), (float, 'str. def'),\n",
    "              (float, 'td avg'), (float, 'td acc'), (float, 'td def'), \n",
    "              (float, 'sub. avg')]\n",
    "    content = sess.get(url).text\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    \n",
    "    # Parse name(s).\n",
    "    try:\n",
    "        stats['name'] = soup.select('.b-content__title-highlight')[0]\\\n",
    "                            .get_text().strip()\n",
    "        name_list = stats['name'].split(' ')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        try:\n",
    "            stats['fname'], stats['lname'] = name_list\n",
    "        except ValueError:\n",
    "            stats['fname'] = name_list[0]\n",
    "            stats['lname'] = ' '.join(name_list[1:])\n",
    "\n",
    "    # Parse nickname.\n",
    "    try:\n",
    "        stats['nick'] = soup.select('.b-content__Nickname')[0].get_text().strip()\n",
    "    except Exception:\n",
    "        stats['nick'] = ''\n",
    "        \n",
    "    # Parse record.\n",
    "    try:\n",
    "        record = re.findall('\\d+', soup.select('.b-content__title-record')[0]\\\n",
    "                            .get_text())\n",
    "    except Exception:\n",
    "        stats['W'], stats['L'], stats['D'] = None*3\n",
    "    else:\n",
    "        try:\n",
    "            stats['W'], stats['L'], stats['D'] = map(int, record)\n",
    "        except ValueError:\n",
    "            stats['W'], stats['L'], stats['D'], stats['NC'] = map(int, record)\n",
    "        \n",
    "    # Parse li fields.\n",
    "    for dtype, attr in fields:\n",
    "        stats[attr] = attr_parser(soup, dtype, attr)\n",
    "    \n",
    "    # Clean up numeric fields.\n",
    "    if stats['height']:\n",
    "        temp = [int(i.strip('\\\"')) for i in stats['height'].split('\\'')]\n",
    "        stats['height'] = temp[0] * 12 + temp[1]\n",
    "    if stats['weight']:\n",
    "        stats['weight'] = int(stats['weight'].split(' ')[0])\n",
    "    if verbose:\n",
    "        print(stats['name'] + ' - scraped')\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T17:04:41.739803Z",
     "start_time": "2019-04-13T17:04:41.734510Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_all(f_urls, limit=None, **kwargs):\n",
    "    \"\"\"Return list of dicts of fighter stats. Set limit to small integer when\n",
    "    testing.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for i, url in enumerate(f_urls[:limit]):\n",
    "        stats = scrape_fighter_page(url, **kwargs)\n",
    "        output.append(stats)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Round {i}: scraped {stats['name']}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T18:10:05.337587Z",
     "start_time": "2019-04-13T18:10:05.334431Z"
    }
   },
   "outputs": [],
   "source": [
    "scrape_file = 'scrape.pkl'\n",
    "df_file = 'scrape.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T18:08:59.351718Z",
     "start_time": "2019-04-13T18:08:59.347635Z"
    }
   },
   "outputs": [],
   "source": [
    "# pages = scrape_fightmetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T18:09:01.469180Z",
     "start_time": "2019-04-13T18:09:01.465753Z"
    }
   },
   "outputs": [],
   "source": [
    "# f_urls = extract_urls(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T18:09:43.410664Z",
     "start_time": "2019-04-13T18:09:43.406387Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fighter data collected 4/3/2019.\n",
    "# data = scrape_all(f_urls, None, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T18:09:09.011106Z",
     "start_time": "2019-04-13T18:09:09.005789Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(scrape_file, 'wb') as f:\n",
    "#     pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T18:09:11.705855Z",
     "start_time": "2019-04-13T18:09:11.687984Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(scrape_file, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T18:10:28.349397Z",
     "start_time": "2019-04-13T18:10:28.251191Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_csv(df_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
